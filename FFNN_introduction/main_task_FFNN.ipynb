{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to feed-forward neural networks\n",
        "\n",
        "---\n",
        "\n",
        "Lecture: \"Physics-augmented machine learning\" @ Cyber-Physical Simulation, TU Darmstadt\n",
        "\n",
        "Lecturer: Prof. Oliver Weeger\n",
        "\n",
        "Assistants: Dr.-Ing. Maximilian Kannapin, Jasper O. Schommartz, Dominik K. Klein\n",
        "\n",
        "Summer term 2024\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### In this notebook, you will...\n",
        "\n",
        "\n",
        "*   Calibrate feed-forward neural networks to different one-dimensional datasets\n",
        "*   Learn the influence of hyperparameters on the calibrated model\n",
        "* Learn the difference between interpolation and extrapolation\n",
        "*   Learn to construct convex and monotonous neural networks\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CAbS69NPtdTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Run the following cell to clone the GitHub repository in your current Google Colab environment.*"
      ],
      "metadata": {
        "id": "-Xin1T39xsu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/CPShub/LecturePhysicsAwareML.git"
      ],
      "metadata": {
        "id": "g_-FEL0BxvLk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0e8b3ed-f98a-41d1-88ca-11dd50b94535"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LecturePhysicsAwareML'...\n",
            "remote: Enumerating objects: 23, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 23 (delta 1), reused 20 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (23/23), 127.72 KiB | 976.00 KiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Run the following cell to import all modules and python files to this notebook. If you made changes in the python files, run the following cell again to update the python files in this notebook.*\n"
      ],
      "metadata": {
        "id": "y-v5FNcbWBpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*If you want to clone the repository again, you have to delete it from your Google Colab files first. For this, you can run the following cell.*"
      ],
      "metadata": {
        "id": "u8QQsUEEvTxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%rm -rf LecturePhysicsAwareML"
      ],
      "metadata": {
        "id": "1nOi7jY3vbsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import datetime\n",
        "now = datetime.datetime.now\n",
        "import LecturePhysicsAwareML.FFNN_introduction.data as ld\n",
        "import LecturePhysicsAwareML.FFNN_introduction.models as lm\n",
        "import LecturePhysicsAwareML.FFNN_introduction.plots as lp"
      ],
      "metadata": {
        "id": "qFy4zsH0WAz6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Nonlinear regression\n",
        "\n",
        "\n",
        "''In a nutshell, feed-forward neural networks (FFNNs) can be seen as a composition of several vector-valued functions, where the components of the vectors are referred to as nodes or neurons, and the function in each neuron is referred to as activation function. More explicitely, the FFNN with the vector-valued inpout $\\boldsymbol{x}=:\\boldsymbol{A}_0\\in\\mathbb{R}^{n^{[0]}}$, output $\\boldsymbol{y}=:\\boldsymbol{A}_{[H+1]}\\in\\mathbb{R}^{n^{[H+1]}}$, and $H$ hidden layers is given by\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "\\boldsymbol{A}_h=\\sigma^{[h]}\\left(\\boldsymbol{w}^T\\boldsymbol{x}+\\boldsymbol{b}\\right)\\in\\mathbb{R}^{n^{[h]}}\\,,\\qquad h=1,...,H+1\\,.\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "Here, $\\boldsymbol{w}^{[h]}\\in\\mathbb{R}^{n^{[h]}\\times n^{[h-1]}}$ are the weights and $\\boldsymbol{b}\\in\\mathbb{R}^{n^{[h]}}$ the bias in each layer. Together, they form the set of parameters $\\boldsymbol{\\phi}$ of the neural network, which is optimized in the calibration process to fit a given dataset. In the layers $\\boldsymbol{A}_h$, the scalar activation functions $\\sigma^{[h]}$ are applied in a component-wise manner.'' (Klein et al., CMAME 400:115501) A FFNN has different hyperparameters. Here, the hyperparameters are given by the number of nodes and layers and the activation functions.\n",
        "\n",
        "<br>\n",
        "\n",
        "After fixing the FFNNs hyperparameters, it can be calibrated to fit a given dataset. In this notebook, we consider datasets of the form\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathcal{D}=\\big\\{(x_i,\\,y_i) \\big\\}_{i=1,...,D}\\,,\\qquad x_i\\in\\mathbb{R},\\,y_i\\in\\mathbb{R}\\,.\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "Consequently, input and output of the FFNNs are also scalar-valued, i.e., $\\mathbb{R}^{n^{[0]}}=\\mathbb{R}^{n^{[H+1]}}=1$. The parameters $\\boldsymbol{\\phi}$ of the FFNN are determined such that it best-approximates the data $\\mathcal{D}$. Typically, this is done by minimizing a loss (or cost) function which provides a measure for the difference between data and FFNN model prediction. Here, the mean-squared error (MSE) is applied:\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "\\operatorname{MSE}(\\boldsymbol{\\phi},\\,\\mathcal{D})=\\frac{1}{D}\\sum_{d=1}^D\\lvert\\lvert \\operatorname{FFNN}(x_i,\\,\\boldsymbol{\\phi})-y_i\\rvert\\rvert_2^2\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "Since this is a highly nonlinear minimisation problem, iterative solution methods are applied. The number of iterations (or epochs) in the optimization process influences the quality of the calibrated model.\n",
        "\n",
        "<br>\n",
        "\n",
        "### Tasks\n",
        "\n",
        "Despite their simple structure, FFNNs can represent basically every continuous function. Here, this is demonstrated by calibrating different FFNNs to different datasets. Furthermore, the influence of model and calibration hyperparameters are investigated.\n",
        "\n",
        "\n",
        "*  Calibrate FFNNs to different datasets ('bathtub', 'curve', 'double_curve')\n",
        "* Vary the number of hidden layers in [1, 2, 3] and the number of nodes in [4, 8, 16].\n",
        "* Vary the number of epochs in the parameter optimization process in [500, 1000, 2500, 3000].\n",
        "* Use different activation functions, e.g., Relu, Softplus and Sigmoid. Use a linear activation function in the output layer.\n",
        "\n",
        "Note that you do not have to evaluate all possible combinations, but only enough to understand the influence of the different hyperparameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "skD3hX3uxA5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#   adapt the model name for your plots\n",
        "model_name = 'test'\n",
        "\n",
        "#   number of nodes in each layer\n",
        "units = [32,32,1]\n",
        "\n",
        "#   activation function in each layer\n",
        "#   options: 'softplus', 'tanh', 'relu', 'linear', ...\n",
        "activation = ['softplus','softplus','linear']\n",
        "\n",
        " #   load model\n",
        "model = lm.main(units=units, activation=activation)\n",
        "\n",
        "#   dataset options: 'bathtub', 'curve', 'double_curve'\n",
        "data = 'curve'\n",
        "\n",
        "#   epochs: number of iterations in the optimisation process\n",
        "epochs = 100\n",
        "\n",
        "#   load data\n",
        "xs, ys, xs_c, ys_c = ld.get_data(data)\n",
        "\n",
        "#   calibrate model\n",
        "t1 = now()\n",
        "print(t1)\n",
        "\n",
        "#   set \"verbose=2\" to observe the progress of the calibration process\n",
        "tf.keras.backend.set_value(model.optimizer.learning_rate, 0.002)\n",
        "h = model.fit([xs_c], [ys_c], epochs = epochs,  verbose = 2)\n",
        "\n",
        "t2 = now()\n",
        "print('it took', t2 - t1, '(sec) to calibrate the model')\n",
        "\n",
        "lp.plot_loss(h)\n",
        "\n",
        "\n",
        "\n",
        "lp.plot_data_model(xs, ys, xs_c, ys_c, model, model_name, data, 4)"
      ],
      "metadata": {
        "id": "19DfAdMzYFeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Convex and monotonous neural networks\n",
        "\n",
        "''To lay the foundational intuition for constructing convex and monotonus neural networks, we first consider the univariate function\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "f:\\mathbb{R}\\rightarrow\\mathbb{R},\\quad x\\mapsto f(x):=(g\\circ h)(x)=g(h(x))\\,,\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "where $f$ is composed of two functions $g,h:\\mathbb{R}\\rightarrow \\mathbb{R}$. Given that all of these functions are twice continuously differentiable, convexity of $f$ in $x$ is equivalent to the nonnegativity of the second derivative\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "f''(x)=(g''\\circ h)(x)\\, h'(x)^2 + (g' \\circ h)(x)\\, h''(x) \\geq 0\\,.\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "A sufficient, albeit not necessary condition for this is that the function $h$ is convex ($h''\\geq 0$), while the function $g$ is convex and nondecreasing (g'\\geq 0,\\,g''\\geq 0$). ... '' (Klein et al., DCE 4:e25)\n",
        "\n",
        "\n",
        "1D example for convexity and monotonicity. Write the conditions for convex and monotonous FFNNs down.\n",
        "\n",
        "<br>\n",
        "\n",
        "Sufficient conditions for convex neural networks are\n",
        "\n",
        "\n",
        "*   A convex activation function in the first hidden layer\n",
        "*   Convex and non-decreasing activation functions in every subsequent layer\n",
        "* Non-negative weights in every layer beside the first one\n",
        "\n",
        "\n",
        "Sufficient conditions for monotonous neural networks are\n",
        "\n",
        "*  Monotonous activation functions in every layer\n",
        "* Non-negative weights in every layer\n",
        "* If at least one layer uses non-convex activation functions, the overall NN is not convex\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "### Tasks\n",
        "\n",
        "* Construct convex FFNNs (C-FFNNs) and monotonous FFNNs (M-FFNNs) by using suitable activation functions and restrictions to the weights of the FFNN.\n",
        "* Calibrate C-FFNNs and M-FFNNs to the datasets introduced above. What do you observe?\n",
        "* For this, consider the following activation functions:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\text{Softplus}:\\qquad \\sigma(x)&=\\log(1+e^x)\n",
        "\\\\\n",
        "\\text{Tanh}:\\qquad \\sigma(x)&=\\operatorname{tanh}(x)=\\frac{e^{2x}-1}{e^{2x}+1}\n",
        "\\\\\n",
        "\\text{Linear}:\\qquad \\sigma(x)&=x\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "* Which of these functions are convex and / or non-decreasing?"
      ],
      "metadata": {
        "id": "li53gWrYxJwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#   number of nodes in each layer\n",
        "units = [32,32,1]\n",
        "\n",
        "#   activation function in each layer\n",
        "#   options: 'softplus', 'tanh', 'relu', 'linear', ...\n",
        "activation = ['softplus','softplus','linear']\n",
        "\n",
        "#   non_neg: restrict the weights in different layers to be non-negative\n",
        "non_neg = [False, False, False]\n",
        "\n",
        "#   dataset options: 'bathtub', 'curve', 'double_curve'\n",
        "data = 'curve'\n",
        "\n",
        "#   epochs: number of iterations in the optimisation process\n",
        "epochs = 100\n",
        "\n",
        " #   load model\n",
        " #  adapt this so that the students don't see the \"non_neg\" part at the first task.\n",
        "\n",
        "model = lm.main_con(units=units, activation=activation, non_neg=non_neg)\n",
        "\n",
        "#   load data\n",
        "\n",
        "xs, ys, xs_c, ys_c = ld.get_data(data)\n",
        "\n",
        "\n",
        "#   calibrate model\n",
        "\n",
        "t1 = now()\n",
        "print(t1)\n",
        "\n",
        "\n",
        "#   set \"verbose=2\" to observe the progress of the calibration process\n",
        "tf.keras.backend.set_value(model.optimizer.learning_rate, 0.002)\n",
        "h = model.fit([xs_c], [ys_c], epochs = epochs,  verbose = 2)\n",
        "\n",
        "t2 = now()\n",
        "print('it took', t2 - t1, '(sec) to calibrate the model')\n",
        "\n",
        "lp.plot_loss(h)\n",
        "\n",
        "\n",
        "# include the type of FFNN used. and show it in the plot title.\n",
        "model_name = 'FFNN'\n",
        "lp.plot_data_model(xs, ys, xs_c, ys_c, model, model_name, data, 4)"
      ],
      "metadata": {
        "id": "QBCtKDNwgjuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Sobolev training\n"
      ],
      "metadata": {
        "id": "ZnvdLfoaZ9SO"
      }
    }
  ]
}